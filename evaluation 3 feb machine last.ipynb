{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What does R-squared represent in a regression model?\n",
        "\n",
        "R-squared: This represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It indicates how well the regression model fits the data. A higher R-squared value means a better fit.\n",
        "2. What are the assumptions of linear regression?\n",
        "Assumptions of Linear Regression:\n",
        "\n",
        "Linearity: The relationship between the independent and dependent variables is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: The variance of residuals is constant across all levels of the independent variable.\n",
        "\n",
        "Normality: Residuals (errors) of the model are normally distrubed.\n",
        "\n"
      ],
      "metadata": {
        "id": "_Rl5irvkoeJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between R-squared and Adjusted R-squared?\n",
        "\n",
        "R-squared vs. Adjusted R-squared: R-squared measures the proportion of variance explained by the model. Adjusted R-squared adjusts this value based on the number of predictors, correcting for the model's complexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "6ENtaN93pdno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why do we use Mean Squared Error (MSE)?   \n",
        "Mean Squared Error (MSE): This measures the average squared difference between observed and predicted values. It helps evaluate the accuracy of a model by penalizing larger errors more heavily.\n",
        "\n",
        "\n",
        " 5. What does an Adjusted R-squared value of 0.85 indicate?\n",
        " Adjusted R-squared value of 0.85: Indicates that approximately 85% of the variance in the dependent variable is explained by the independent variables, adjusted for the number of predictors in the model.\n",
        "\n",
        "6. How do we check for normality of residuals in linear regression? Checking Normality of Residuals: Use visual tools like Q-Q plots or statistical tests like the Shapiro-Wilk test. The residuals should be approximately normally distributed.\n",
        "\n",
        "7. What is multicollinearity, and how does it impact regression?\n",
        "Multicollinearity: This occurs when independent variables in a regression model are highly correlated, causing issues with the stability and interpretability of the model. It can inflate the variance of coefficient estimates\n",
        "\n",
        " 8. What is Mean Absolute Error (MAE)?\n",
        " Mean Absolute Error (MAE): This measures the average absolute difference between observed and predicted values. Unlike MSE, it does not penalize larger errors more heavily.\n",
        "\n",
        "9. What are the benefits of using an ML pipeline?\n",
        "Benefits of Using an ML Pipeline:\n",
        "\n",
        "Streamlines the workflow by integrating data preprocessing, model training, and evaluation.\n",
        "\n",
        "Enhances reproducibility and consistency.\n",
        "\n",
        "Simplifies model management and deployment\n",
        "\n",
        " 10. Why is RMSE considered more interpretable than MSE?\n",
        "\n",
        " RMSE vs. MSE: RMSE (Root Mean Squared Error) is the square root of MSE and is in the same units as the dependent variable. It is considered more interpretable because it directly relates to the scale of the data.\n",
        "\n",
        "11. What is pickling in Python, and how is it useful in ML?\n",
        "\n",
        "Pickling in Python: Pickling serializes Python objects into a byte stream, allowing them to be saved to a file and later loaded back. It is useful in ML for saving trained models and other objects.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KmeybaZvp3MS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "12. What does a high R-squared value mean?\n",
        "High R-squared Value: Indicates that a large proportion of the variance in the dependent variable is explained by the independent variables in the model, suggesting a good fit.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 13. What happens if linear regression assumptions are violated?\n",
        " Violating Linear Regression Assumptions: Can lead to biased estimates, inefficiency, and invalid inference. For example, if residuals are not normally distributed, hypothesis tests may not be valid.\n",
        "\n",
        "\n",
        "\n",
        "  14. How can we address multicollinearity in regression?\n",
        "\n",
        "  Addressing Multicollinearity:\n",
        "\n",
        "Remove or combine highly correlated predictors.\n",
        "\n",
        "Use regularization techniques like Ridge or Lasso regression.\n",
        "\n",
        "Apply Principal Component Analysis (PCA) to reduce dimensionality.\n",
        "\n",
        "\n",
        "  \n",
        "   15. How can feature selection improve model performance in regression analysis?\n",
        "\n",
        "   Feature Selection in Regression Analysis: Improves model performance by removing irrelevant or redundant predictors, reducing overfitting, and enhancing model interpretability.\n",
        "\n"
      ],
      "metadata": {
        "id": "lWhFA3uYr3iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How is Adjusted R-squared calculated?\n",
        "Adjusted R-squared Calculation:\n",
        "\n",
        "Adjusted¬†R\n",
        "2\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëÖ\n",
        "2\n",
        ")\n",
        "(\n",
        "ùëõ\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "ùëõ\n",
        "‚àí\n",
        "ùëò\n",
        "‚àí\n",
        "1\n",
        "where\n",
        "ùëõ\n",
        " is the number of observations and\n",
        "ùëò\n",
        " is the number of predictors.\n",
        "\n",
        "\n",
        "\n",
        "17. Why is MSE sensitive to outliers?\n",
        "MSE Sensitivity to Outliers: MSE squares the errors, which means larger errors have a disproportionately higher impact, making it sensitive to outliers.\n",
        "\n",
        "\n",
        "\n",
        " 18. What is the role of homoscedasticity in linear regression?\n",
        " Homoscedasticity in Linear Regression: Assumes that the variance of residuals is constant across levels of the independent variable. If violated (heteroscedasticity), it can affect the efficiency and validity of the model.\n",
        "\n",
        "\n",
        "\n",
        " 19. What is Root Mean Squared Error (RMSE)?\n",
        " Root Mean Squared Error (RMSE): Measures the average magnitude of the residuals, giving an indication of the absolute fit of the model to the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  20. Why is pickling considered risky?\n",
        "  Risks of Pickling: Security risks if loading pickled data from untrusted sources, as it can execute arbitrary code during deserialization.\n",
        "\n",
        "\n",
        "  \n",
        "  21. What alternatives exist to pickling for saving ML models?\n",
        "  Alternatives to Pickling for Saving ML Models:\n",
        "\n",
        "Joblib: Efficiently saves large data objects.\n",
        "\n",
        "HDF5: A file format and set of tools for storing complex data.\n",
        "\n",
        "ONNX: Open Neural Network Exchange format for interoperability.\n",
        "\n",
        "\n",
        "  \n",
        "   22. What is heteroscedasticity, and why is it a problem?\n",
        "   Heteroscedasticity: When the variance of residuals is not constant. It can lead to inefficient estimates and incorrect conclusions in hypothesis testing\n",
        "    23. How can interaction terms enhance a regression model's predictive power?\n",
        "    Interaction Terms: Allow the model to capture complex relationships between predictors, enhancing predictive power and interpretability."
      ],
      "metadata": {
        "id": "PiHDnLtpsaC4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yUyJfCUoP7O"
      },
      "outputs": [],
      "source": [
        "##practical ##question"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model using Seaborn's \"diamonds' dataset.\n",
        "\n",
        "\n",
        "2. Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for a linear regression model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQUQB7SMuMbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select relevant features and target\n",
        "X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Plot the distribution of residuals\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.xlabel('Residuals')\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RubPgEUUvDzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f'MSE: {mse}')\n",
        "print(f'MAE: {mae}')\n",
        "print(f'RMSE: {rmse}')\n"
      ],
      "metadata": {
        "id": "oc0hRVwCvO4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Scatter plot to check linearity\n",
        "for feature in X_train.columns:\n",
        "    plt.scatter(X_train[feature], y_train)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Price')\n",
        "    plt.title(f'Linearity check: {feature} vs. Price')\n",
        "    plt.show()\n",
        "\n",
        "# Residuals plot for homoscedasticity\n",
        "sns.residplot(x=y_pred, y=residuals, lowess=True)\n",
        "plt.xlabel('Predicted values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs. Predicted values')\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix for multicollinearity\n",
        "corr_matrix = X_train.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AYSG-EFvvJ9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create a pipeline with feature scaling and linear regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Evaluate the linear regression model\n",
        "scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
        "print(f'Linear Regression R^2: {np.mean(scores)}')\n",
        "\n",
        "# Evaluate the Ridge regression model\n",
        "pipeline.set_params(regressor=Ridge())\n",
        "scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
        "print(f'Ridge Regression R^2: {np.mean(scores)}')\n",
        "\n",
        "# Evaluate the Lasso regression model\n",
        "pipeline.set_params(regressor=Lasso())\n",
        "scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
        "print(f'Lasso Regression R^2: {np.mean(scores)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "Tcj0BOFgvu2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print coefficients, intercept, and R-squared score\n",
        "print(f'Coefficients: {model.coef_}')\n",
        "print(f'Intercept: {model.intercept_}')\n",
        "print(f'R-squared: {model.score(X_test, y_test)}')\n"
      ],
      "metadata": {
        "id": "zNEkohOFv1Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the tips dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Define the feature and target\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict the values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('Total Bill')\n",
        "plt.ylabel('Tip')\n",
        "plt.title('Total Bill vs. Tip')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HO1wGB2Jvlvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X + np.random.randn(100, 1) * 2\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict new values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data points and regression line\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Synthetic Data: Linear Regression')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G2B0tEy2wA7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Save the model to a file\n",
        "with open('linear_regression_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n"
      ],
      "metadata": {
        "id": "qxuw4LE2t5Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7WKItsbwYOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X**2 + np.random.randn(100, 1) * 10\n",
        "\n",
        "# Transform features to polynomial\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict the values\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-sEE1MssweqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X + np.random.randn(100, 1) * 2\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the model's coefficient and intercept\n",
        "print(f'Coefficient: {model.coef_[0][0]}')\n",
        "print(f'Intercept: {model.intercept_[0]}')\n"
      ],
      "metadata": {
        "id": "DD-GusfHwySa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X**2 + np.random.randn(100, 1) * 10\n",
        "\n",
        "# Fit polynomial regression models of different degrees\n",
        "degrees = [1, 2, 3, 4]\n",
        "for degree in degrees:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "    y_pred = model.predict(X_poly)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    print(f'Degree {degree} - MSE: {mse}')\n"
      ],
      "metadata": {
        "id": "uV8Tk2Z8w96D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select relevant features and target\n",
        "X = diamonds[['carat', 'depth']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the linear regression"
      ],
      "metadata": {
        "id": "gtmH4K0Fw5iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features.\n",
        "\n",
        "\n",
        "\n",
        "15. Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve.\n",
        "\n",
        "16. Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score."
      ],
      "metadata": {
        "id": "NoqspvJNxcAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load dataset\n",
        "data = load_boston()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Calculate VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['feature'] = df.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
        "\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "KW5Pw1HLxz8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X**4 - 3 * X**3 + 2 * X**2 + X + np.random.randn(100, 1) * 10\n",
        "\n",
        "# Transform features to polynomial (degree 4)\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict the values\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Polynomial Regression (Degree 4)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xev_N2Arx6Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset (using Seaborn's diamonds dataset as an example)\n",
        "import seaborn as sns\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Print the R-squared score\n",
        "r_squared = pipeline.score(X_test, y_test)\n",
        "print(f'R-squared: {r_squared}')\n"
      ],
      "metadata": {
        "id": "H-3MtAknyAzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X**3 - 3 * X**2 + 2 * X + np.random.randn(100, 1) * 10\n",
        "\n",
        "# Transform features to polynomial (degree 3)\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict the values\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EiVV0P2kxN6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5)\n",
        "y = 3 + 2*X[:,0] + 4*X[:,1] + 5*X[:,2] + 6*X[:,3] + 7*X[:,4] + np.random.randn(100)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print R-squared score and coefficients\n",
        "print(f'R-squared: {model.score(X_test, y_test)}')\n",
        "print(f'Coefficients: {model.coef_}')\n",
        "print(f'Intercept: {model.intercept_}')\n"
      ],
      "metadata": {
        "id": "VQyADhUPyjmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X + np.random.randn(100, 1) * 2\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict new values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data points and regression line\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Synthetic Data: Linear Regression')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NWoHhKTayoIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3)\n",
        "y = 3 + 2*X[:,0] + 4*X[:,1] + 5*X[:,2] + np.random.randn(100)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print R-squared score and coefficients\n",
        "print(f'R-squared: {model.score(X_test, y_test)}')\n",
        "print(f'Coefficients: {model.coef_}')\n",
        "print(f'Intercept: {model.intercept_}')\n"
      ],
      "metadata": {
        "id": "H_8Lq9hKyrwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Save the model to a file using joblib\n",
        "joblib.dump(model, 'linear_regression_model.joblib')\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load('linear_regression_model.joblib')\n",
        "\n",
        "# Print the loaded model's coefficients and intercept\n",
        "print(f'Coefficients: {loaded_model.coef_}')\n",
        "print(f'Intercept: {loaded_model.intercept_}')\n"
      ],
      "metadata": {
        "id": "r4T-ljwByvfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the tips dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# One-hot encode categorical features\n",
        "encoder = OneHotEncoder(drop='first')\n",
        "encoded_features = encoder.fit_transform(tips[['sex', 'smoker', 'day', 'time']]).toarray()\n",
        "\n",
        "# Create DataFrame for encoded features\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['sex', 'smoker', 'day', 'time']))\n",
        "\n",
        "# Combine encoded features with numerical features\n",
        "X = pd.concat([tips[['total_bill', 'size']], encoded_df], axis=1)\n",
        "y = tips['tip']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print R-squared score\n",
        "print(f'R-squared: {model.score(X_test, y_test)}')\n"
      ],
      "metadata": {
        "id": "Y158YobUy-jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X + np.random.randn(100, 1) * 2\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit Linear Regression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "# Fit Ridge Regression\n",
        "ridge_reg = Ridge(alpha=1.0)\n",
        "ridge_reg.fit(X_train, y_train)\n",
        "\n",
        "# Print coefficients and R-squared scores\n",
        "print(f'Linear Regression - Coeff"
      ],
      "metadata": {
        "id": "xZzWjSKmzDNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WRW3JLR1yInB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}