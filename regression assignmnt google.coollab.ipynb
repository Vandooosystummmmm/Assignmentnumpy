{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What does R-squared represent in a regression model?\n",
        "Ans:- R-squared: Represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It indicates the goodness of fit of the model.\n",
        "\n",
        "  2. What are the assumptions of linear regression?\n",
        "  Ans :- Assumptions of Linear Regression:\n",
        "\n",
        "Linearity: The relationship between the independent and dependent variables is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: Constant variance of the errors.\n",
        "\n",
        "Normality: The residuals of the model are normally distributed.\n",
        "\n",
        "No multicollinearity: Independent variables are not highly"
      ],
      "metadata": {
        "id": "jK98GogQYBEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between R-squared and Adjusted R-squared?\n",
        "ans :- R-squared vs. Adjusted R-squared:\n",
        "\n",
        "R-squared: Measures the proportion of variance explained by the model.\n",
        "\n",
        "Adjusted R-squared: Adjusts the R-squared value for the number of predictors in the model, providing a more accurate measure of goodness of fit, especially when more variables are added"
      ],
      "metadata": {
        "id": "SMRMaM2TZIGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why do we use Mean Squared Error (MSE)?\n",
        "Ans:- Mean Squared Error (MSE): A measure of the average squared difference between the observed actual outcomes and the outcomes predicted by the model. It's used to assess the accuracy of a model.\n",
        "\n",
        "\n",
        " 5. What does an Adjusted R-squared value of 0.85 indicate\n",
        " Ans ;- Adjusted R-squared value of 0.85: Indicates that 85% of the variance in the dependent variable is explained by the independent variables, adjusted for the number of predictors.\n",
        "\n"
      ],
      "metadata": {
        "id": "n3x6v20CZZGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we check for normality of residuals in linear regression?  \n",
        "ans. Check for normality of residuals: Can be done using visual methods like Q-Q plots, histograms of residuals, or statistical tests like the Shapiro-Wilk test.\n",
        "\n",
        "\n",
        "7. What is multicollinearity, and how does it impact regression?\n",
        "Multicollinearity: A situation in which two or more predictor variables in a regression model are highly correlated. It can lead to unreliable estimates of regression coefficients.\n",
        "\n",
        "\n",
        "8. What is Mean Absolute Error (MAE)?\n",
        "ans;- Mean Absolute Error (MAE): A measure of the average absolute difference between the actual values and the predicted values. It is less sensitive to outliers compared to MSE.\n",
        "\n",
        " 9. What are the benefits of using an ML pipeline?\n",
        " Ans:- Benefits of using an ML pipeline:\n",
        "\n",
        "Streamlines the process of preparing data, training models, and evaluating results.\n",
        "\n",
        "Reduces the risk of data leakage.\n",
        "\n",
        "Facilitates reproducibility and maintainability of the code.\n",
        "\n",
        "\n",
        "  10. Why is RMSE considered more interpretable than MSE?\n",
        "   RMSE vs. MSE: RMSE (Root Mean Squared Error) is the square root of MSE, making it easier to interpret as it is in the same units as the target variable.\n",
        "  \n",
        "   11. What is pickling in Python, and how is it useful in ML?\n",
        "   Pickling in Python: A way to serialize and deserialize Python objects, which is useful for saving machine learning models to disk and later loading them for inference.\n",
        "\n",
        " 12. What does a high R-squared value mean?\n",
        " High R-squared value: Indicates a good fit, meaning the model explains a large portion of the variance in the dependent variable.\n",
        "\n",
        "\n",
        "  13. What happens if linear regression assumptions are violated?\n",
        "  Violations of linear regression assumptions: Can lead to biased or inefficient estimates and affect the validity of the model's predictions and conclusions.\n",
        "\n",
        "\n",
        "  14. How can we address multicollincarity in regression?\n",
        "  Addressing multicollinearity:\n",
        "\n",
        "Remove or combine highly correlated predictors.\n",
        "\n",
        "Use regularization techniques like Ridge Regression or Lasso.\n",
        "\n",
        "Apply Principal Component Analysis (PCA).\n",
        "\n",
        "\n",
        "   15. Why do we use pipelines in machine learning?\n",
        "   Using pipelines in ML: Ensures a seamless and consistent workflow from data preprocessing to model training and evaluation.\n",
        "     16. How is Adjusted R-squared calculated?\n",
        "     Adjusted R-squared calculation: Adjusted\n",
        "ùëÖ\n",
        "2\n",
        " = 1 - [(1 -\n",
        "ùëÖ\n",
        "2\n",
        ")(n - 1)/(n - p - 1)], where\n",
        "ùëõ\n",
        " is the number of observations and\n",
        "ùëù\n",
        " is the number of predictors.\n",
        "\n",
        "17. Why is MSE sensitive to outliers?\n",
        "MSE sensitivity to outliers: Due to the squaring of errors, MSE gives more weight to larger errors, making it sensitive to outliers.\n",
        "18. What is the role of homoscedasticity in linear regression? Role of homoscedasticity in linear regression: Ensures that the variance of errors is constant across all levels of the independent variables, a key assumption for valid inference.\n",
        "\n",
        "\n",
        "19. What is Root Mean Squared Error (RMSE)?  Root Mean Squared Error (RMSE): The square root of the average of the squared differences between the predicted and actual values. It provides an indication of the model's prediction error in the same units as the target variable.\n",
        "\n",
        "\n",
        "20. Why is pickling considered risky? Pickling risks: Can lead to security vulnerabilities if untrusted data is loaded, as it can execute arbitrary code during unpickling.\n",
        "\n",
        "21. What alternatives exist to pickling for saving ML models?\n",
        "Alternatives to pickling:\n",
        "\n",
        "Joblib: Efficiently serializes large data arrays and models.\n",
        "\n",
        "HDF5: Used for storing large amounts of numerical data.\n",
        "\n",
        "ONNX: Open Neural Network Exchange for storing ML models.\n",
        "\n",
        "22. What is heteroscedasticity, and why is it a problem?   \n",
        "Heteroscedasticity: The presence of non-constant variance in the error terms. It can lead to inefficient estimates and affect the validity of hypothesis tests.\n",
        " 23. How does adding irrelevant predictors affect R-squared and Adjusted R-squared?\n",
        " Impact of irrelevant predictors: Adding irrelevant predictors can inflate\n",
        "ùëÖ\n",
        "2\n",
        " without improving model fit, but Adjusted\n",
        "ùëÖ\n",
        "2\n",
        " accounts for this by penalizing the addition of irrelevant variables."
      ],
      "metadata": {
        "id": "zV5Z1i7IaEcd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By72MliSX-0s"
      },
      "outputs": [],
      "source": []
    }
  ]
}